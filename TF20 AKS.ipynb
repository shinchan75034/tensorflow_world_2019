{"cells":[{"cell_type":"markdown","source":["#### Notebook: Aircraft Maintenance Log Text Clasification\nDeploy Custom Build ML Model as a Web Service in Azure Container Instance and Kubernetes\n\nVersion 1.0"],"metadata":{}},{"cell_type":"markdown","source":["* *Model Registration and Image Deployment:* https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where\n* *Azure Kubernetes Web Service:* https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py\n* *Azure ML Documentation:* https://docs.microsoft.com/en-us/azure/machine-learning/service/\n* *Reference Link:* https://docs.databricks.com/user-guide/libraries.html and add the below string as your **PyPi package**\n* Provide this full string: **azureml-sdk[databricks]**\n* You can select the option to attach the library to all clusters or just one cluster."],"metadata":{}},{"cell_type":"markdown","source":["We have the model created in previous notebook. Now we are ready to deploy it. This notebook assumes the user subscription can create Kubernetes clusters. The major outline of this notebook is:\n\n1. Generate small test data\n2. Create a scoring file - exactly two functions - `init()` and `run()`\n3. Generate docker image of the scoring file\n4. Register the image.\n5. Serve it as a web service (first in Container Instance and then in Kubernetes)"],"metadata":{}},{"cell_type":"code","source":["%python\nlogdata = sqlContext.read.parquet(\"dbfs:/mnt/<MY MOUNT POINT>\")\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\npd_log_data = logdata.toPandas()\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest = train_test_split(pd_log_data, test_size = 0.000005, random_state = 2) #  5 records for quick test"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.6/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n  warnings.warn(&#34;pyarrow.open_stream is deprecated, please use &#34;\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["%sh \nmkdir /databricks/driver/tmp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["dbutils.fs.mount(\nsource = \"wasbs://<container name>@<storage account name>.blob.core.windows.net\",\nmount_point = \"/mnt/tf20\",\nextra_configs = {\"fs.azure.account.key.aademo.blob.core.windows.net\":dbutils.secrets.get(scope = \"<MY SCOPE>\", key = \"<MY KEY>\")})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3149201061332342&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;wasbs://tf20modelassets@aademo.blob.core.windows.net&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/tf20&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> extra_configs = {&#34;fs.azure.account.key.aademo.blob.core.windows.net&#34;:dbutils.secrets.get(scope = &#34;newscope2&#34;, key = &#34;DbksStorageKey&#34;)})\n</span>\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1571105534159-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    306</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    307</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 308</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    309</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    310</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o248.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/tf20; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/tf20\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:465)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/tf20\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:307)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:201)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:299)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:277)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:44)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:61)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:61)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:40)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:272)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:265)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:386)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:333)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:39)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:509)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:509)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:419)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:277)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:153)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:272)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:265)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:153)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:268)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:183)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["%fs ls dbfs:/mnt/tf20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/tf20/Xtest.npy</td><td>Xtest.npy</td><td>141384928</td></tr><tr><td>dbfs:/mnt/tf20/Xtrain.npy</td><td>Xtrain.npy</td><td>565536128</td></tr><tr><td>dbfs:/mnt/tf20/classification_report.csv</td><td>classification_report.csv</td><td>27152</td></tr><tr><td>dbfs:/mnt/tf20/cnn_20191013-021616_ckpt.h5</td><td>cnn_20191013-021616_ckpt.h5</td><td>408866044</td></tr><tr><td>dbfs:/mnt/tf20/dl_confusion.csv</td><td>dl_confusion.csv</td><td>506767</td></tr><tr><td>dbfs:/mnt/tf20/index2tgt.pickle</td><td>index2tgt.pickle</td><td>17492</td></tr><tr><td>dbfs:/mnt/tf20/index2word.pickle</td><td>index2word.pickle</td><td>3210328</td></tr><tr><td>dbfs:/mnt/tf20/stopword_set.pickle</td><td>stopword_set.pickle</td><td>8506</td></tr><tr><td>dbfs:/mnt/tf20/tgt2index.pickle</td><td>tgt2index.pickle</td><td>17492</td></tr><tr><td>dbfs:/mnt/tf20/word2index.pickle</td><td>word2index.pickle</td><td>3210328</td></tr><tr><td>dbfs:/mnt/tf20/ytest.npy</td><td>ytest.npy</td><td>1542508296</td></tr><tr><td>dbfs:/mnt/tf20/ytrain.npy</td><td>ytrain.npy</td><td>6169997888</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"code","source":["dbutils.fs.cp('dbfs:/mnt/tf20/cnn_20191013-021616_ckpt.h5','file:/databricks/driver/tmp/cnn_20191013-021616_ckpt.h5')\ndbutils.fs.cp('dbfs:/mnt/tf20/word2index.pickle','file:/databricks/driver/tmp/word2index.pickle')\ndbutils.fs.cp('dbfs:/mnt/tf20/index2word.pickle','file:/databricks/driver/tmp/index2word.pickle')\ndbutils.fs.cp('dbfs:/mnt/tf20/stopword_set.pickle','file:/databricks/driver/tmp/stopword_set.pickle')\ndbutils.fs.cp('dbfs:/mnt/tf20/index2tgt.pickle','file:/databricks/driver/tmp/index2tgt.pickle')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: True</div>"]}}],"execution_count":8},{"cell_type":"code","source":["%sh ls -lrt /databricks/driver/tmp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 405592\n-rw-r--r-- 1 root root 408866044 Oct 18 16:01 cnn_20191013-021616_ckpt.h5\n-rw-r--r-- 1 root root   3210328 Oct 18 16:01 word2index.pickle\n-rw-r--r-- 1 root root   3210328 Oct 18 16:01 index2word.pickle\n-rw-r--r-- 1 root root      8506 Oct 18 16:01 stopword_set.pickle\n-rw-r--r-- 1 root root     17492 Oct 18 16:01 index2tgt.pickle\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Lets find the model and its assets."],"metadata":{}},{"cell_type":"code","source":["SHARE_ROOT = \"/databricks/driver/tmp/\"\n# the model in h5 format\nMODEL = 'cnn_20191013-021616_ckpt.h5' # Lets use this one from today\nMODEL_PATH = SHARE_ROOT + LSTM_MODEL\nWORD2INDEX = 'word2index.pickle'\nWORD2INDEX_PATH = SHARE_ROOT + WORD2INDEX\nINDEX2WORD = 'index2word.pickle'\nINDEX2WORD_PATH = SHARE_ROOT + INDEX2WORD\nSTOPWORD_SET = 'stopword_set.pickle'\nSTOPWORD_SET_PATH = SHARE_ROOT + STOPWORD_SET\nINDEX2TGT = 'index2tgt.pickle'\nINDEX2TGT_PATH = SHARE_ROOT + INDEX2TGT"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Lets load libraries we need"],"metadata":{}},{"cell_type":"code","source":["#from keras.models import load_model\n#from keras.preprocessing import sequence\nimport tensorflow as tf\nfrom random import randint\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport pickle\nimport re\nimport json\nprint(tf.__version__)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2.0.0-beta1\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Lets create the two functions for scoring file: `init()` and `run()`. `init()` function is where the model and assets are loaded. run() function is where data engineering steps and scoring (predict) will be executed.\n\n##Testing web service deployments\nTo test a web service deployment, you can use the run method of the Webservice object. In the following example, a JSON document is set to a web service and the result is displayed. The data sent must match what the model expects. In this example, the data format matches the input expected by the diabetes model."],"metadata":{}},{"cell_type":"code","source":["def init():\n    # read in the model file\n    global model\n    global word2index\n    global index2word\n    global stopword_set\n    global index2tgt\n    \n    # load model\n    model = tf.keras.models.load_model(MODEL_PATH)\n    print(\"Model Loaded\")\n    \n    # Load dictionary\n    with open(WORD2INDEX_PATH, 'rb') as handle:\n        word2index = pickle.load(handle)\n        print(\"word2index dictionary loaded\", type(word2index))\n    # Load reverse dictionary    \n    with open(INDEX2WORD_PATH, 'rb') as handle:\n        index2word = pickle.load(handle)\n        print(\"index2word dictionary loaded\",type(index2word))\n    # Load stopword_set    \n    with open(STOPWORD_SET_PATH, 'rb') as handle:\n        stopword_set = pickle.load(handle)\n        print(\"stopword_set loaded\", type(stopword_set))\n    # Load index2tgt for reverse lookup of prediction\n    with open(INDEX2TGT_PATH, 'rb') as handle:\n        index2tgt = pickle.load(handle)\n        print(\"index2tgt loaded\", type(index2tgt))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["\ndef run(raw_data):\n    import pandas as pd\n    \n    try:\n        # Read json input\n        data = json.loads(raw_data)['data']\n        dataf = pd.read_json(data, orient='records')\n        holdout_df = dataf.copy()\n        \n        ## parse test data to list\n        interim_data_list = holdout_df['OPEN_FMR_TXT'].tolist()\n        \n        def replace_characters_and_stops(input_sentence, to_be_replaced, replacement_string, stops):\n            # input_sentence is a sentence from a list. to_be_replaced is a string, replacement_string is the unicode replacement. \n            # first we replace special characters with space, then trim long space to one space, then convert to lower case.\n            working_sentence = input_sentence.translate ({ord(c): replacement_string for c in to_be_replaced})\n            fixed_trimmed = re.sub('\\s+', ' ', working_sentence).strip().lower()\n            filtered_sentence = ' '.join([word for word in fixed_trimmed.split() if word not in stops])\n            \n            return filtered_sentence\n        \n        cleaned_data = [] \n        for record in interim_data_list:\n            fixed_record = replace_characters_and_stops(record,  '!@#$%^&*()[]{};:,./<>?\\|`~-=_+', ' ', stopword_set)\n            cleaned_data.append(fixed_record) \n        #print(cleaned_data)\n        # Need nltk.word_tokenize to tokenize each word, then map that word to my dictionary word2index[word] to find it's value.\n        num_recs = len(cleaned_data)\n        MAX_SENTENCE_LENGTH = 200 # The model is trained to accept this input length.\n        X = np.empty((num_recs, ), dtype=list)\n        \n        i = 0\n        nltk.download('punkt')\n        for idx, sentence in enumerate(cleaned_data):\n            \n            words = nltk.word_tokenize(''.join(sentence))\n            \n            seqs = []\n            \n            for word in words:\n                if word in word2index:\n                    seqs.append(word2index[word])\n                    \n                else:\n                    seqs.append(word2index['UNK'])\n                    \n            X[i] = seqs\n            i += 1\n        \n        X = tf.keras.preprocessing.sequence.pad_sequences(X[X !=np.array(None)].tolist(), maxlen=MAX_SENTENCE_LENGTH)\n        \n        # Make prediction here\n        predicted_val3 = model.predict(X)\n        \n        # \n        value3_for_reverse_lookup = predicted_val3.argmax(axis=1)\n        predicted_ata_code3 = np.vectorize(index2tgt.get)(value3_for_reverse_lookup)   \n        zipped = list(zip(interim_data_list, predicted_ata_code3.tolist()))\n        df = pd.DataFrame(zipped, columns = ['OPEN_FMR_TXT', 'PREDICTED_OPEN_ORIGINAL_ATA']) \n        df2json = df.to_json(orient='records')       \n        return df2json\n\n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["Testing `init()` to make sure it loads the model and assets properly"],"metadata":{}},{"cell_type":"code","source":["init()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model Loaded\nword2index dictionary loaded &lt;class &#39;dict&#39;&gt;\nindex2word dictionary loaded &lt;class &#39;dict&#39;&gt;\nstopword_set loaded &lt;class &#39;set&#39;&gt;\nindex2tgt loaded &lt;class &#39;dict&#39;&gt;\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Now we will test the `run()` function. In this step, since we want it to be a web service, we need to convert input data into JSON format, then call the `run()` function."],"metadata":{}},{"cell_type":"code","source":["pred=run(json.dumps({\"data\": Xtest.to_json(orient='records')}))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["Lets take a look at the prediction output. Prediction looks good. pred is json string with structure as expected. The init and run functions work properly. Tested with gpu_5p1mlbeta cluster."],"metadata":{}},{"cell_type":"code","source":["pred"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: &#39;[{&#34;OPEN_FMR_TXT&#34;:&#34;5-983NEF-C                          REF.2530DN                  *NEF* GALLEY CART DOORS\\\\/HANDLES\\\\/DIVIDERS                         AFT GALLEY RAIL NEEDS TO BE REATTACHED.                         MEL AUTHORIZED BY 317887&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;2530&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;A\\\\/C REQUIRES ETOPS PRE-DEPARTURE CK&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ETOPS PDC REQUIRED&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ZIPPER ON CARGO LINER BROKEN ON P9 6R&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;5010&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;6-601NEF-C                          REF.4420AN                  *NEF* PSGR AUDIO\\\\/VIDEO..SYSTEM\\\\/ZONE.....                         IFE SYSTEM INOP SCREEN STUCK ON INITIALIZING                    MEL AUTHORIZED BY 190545&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;4420&#34;}]&#39;</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["#### Workspace for Hosting the Model as a Web Service\nNow we are ready to designate resources for creating a web service, container image, container instance, and Kubernetes clusters."],"metadata":{}},{"cell_type":"code","source":["#Replace this with your own subscription ID\nsbscrptn = '<AZURE SUBSCRIPTION ID>'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from azureml.core import Workspace\nws = Workspace.create(name = 'aa_runtest_aml_workspace_demo', # or another name of your choosing\n                      subscription_id=sbscrptn,\n                      resource_group='tf20_deploy_rg4', # or another name of your choosing\n                      create_resource_group=True,\n                      location='eastus2' # or other supported Azure region\n                     )\nprint(ws.name, \"created.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FHGZFSXZF to authenticate.\nInteractive authentication successfully completed.\nUserWarning: The resource group doesn&#39;t exist or was not provided. AzureML SDK is creating a resource group=tf20_deploy_rg4 in location=eastus2 using subscription=b81e8c4c-2584-4aa7-8b10-bd1099cf015d.\nDeploying KeyVault with name aarunteskeyvault10ddaf60.\nDeploying StorageAccount with name aaruntesstorage377bb15e8.\nDeploying AppInsights with name aaruntesinsights133a7d69.\nDeployed AppInsights with name aaruntesinsights133a7d69. Took 12.69 seconds.\nDeployed KeyVault with name aarunteskeyvault10ddaf60. Took 20.66 seconds.\nDeployed StorageAccount with name aaruntesstorage377bb15e8. Took 33.08 seconds.\nDeploying Workspace with name aa_runtest_aml_workspace_demo.\nDeployed Workspace with name aa_runtest_aml_workspace_demo. Took 58.78 seconds.\naa_runtest_aml_workspace_demo created.\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from azureml.core import Workspace\n\n# Save the workspace config\nws.write_config()\n\n# Reconnect to the workspace (if you're not already signed in, you'll be prompted to authenticate with a code as before)\nws = Workspace.from_config()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["###Persist model assets\nNext we persist the assets we have created for use in operationalization. The conda dependencies are defined in YAML file which we are going to create. This will be used to tell the webservice server which python packages are required to run this web service"],"metadata":{}},{"cell_type":"code","source":["# use azureml to create yml.\nfrom azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_tensorflow_pip_package(core_type='cpu', version='2.0.0')\nmyenv.add_conda_package(\"nltk\")\n\n\nenv_file = \"env_tf20.yml\"\n\nwith open(env_file,\"w\") as f:\n    f.write(myenv.serialize_to_string())\nprint(\"Saved dependency info in\", env_file)\n\nwith open(env_file,\"r\") as f:\n    print(f.read())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Saved dependency info in env_tf20.yml\n# Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n    # Required packages for AzureML execution, history, and data preparation.\n  - azureml-defaults\n\n  - tensorflow==2.0.0\n- nltk\nchannels:\n- conda-forge\n\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["YML file needs to be in same directory as model and assets."],"metadata":{}},{"cell_type":"code","source":["%sh cp env_tf20.yml /databricks/driver/tmp/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["%sh cat env_tf20.yml"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n    # Required packages for AzureML execution, history, and data preparation.\n  - azureml-defaults\n\n  - tensorflow==2.0.0\n- nltk\nchannels:\n- conda-forge\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["The score.py file is python code defining the web service operation. It includes both the `init()` and `run()` functions defined earlier imports the required libraries. These should be nearly identical to the previous defined versions.\n\nThe execution script receives data submitted to a deployed image, and passes it to the model. It then takes the response returned by the model and returns that to the client. The script is specific to your model; it must understand the data that the model expects and returns. The script usually contains two functions that load and run the model:\n\n`init()`: Typically this function loads the model into a global object. This function is run only once when the Docker container is started.\n\n`run(input_data)`: This function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization. You can also work with raw binary data. You can transform the data before sending to the model, or before returning to the client.\n\n##Important:\n\nModel loading in `score.py` `init()` function needs to use `azureml.core.Model`. Then use Keras `load_model` function. It's a two-steps process to load a model:\n    ```\n    \n    from azureml.core.model import Model\n    from keras.models import load_model\n    model_path = Model.get_model_path(model_name = LSTM_MODEL)\n    model = load_model(model_path)\n    print(\"Model Loaded\")```"],"metadata":{}},{"cell_type":"code","source":["%%writefile {SHARE_ROOT}score.py\nimport tensorflow as tf\nfrom azureml.core.model import Model\nfrom random import randint\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport pickle\nimport re\nimport json\n\nMODEL = 'cnn_20191013-021616_ckpt.h5'\nWORD2INDEX = 'word2index.pickle'\nINDEX2WORD = 'index2word.pickle'\nSTOPWORD_SET = 'stopword_set.pickle'\nINDEX2TGT = 'index2tgt.pickle'\n\ndef init():\n    # read in the model file\n    global model\n    global word2index\n    global index2word\n    global stopword_set\n    global index2tgt\n    \n    # load model\n    model_path = Model.get_model_path(model_name = MODEL)\n    model = tf.keras.models.load_model(model_path)\n    print(\"Model Loaded\")\n    \n    # Load dictionary\n    model_path = Model.get_model_path(model_name = WORD2INDEX)\n    with open(model_path, 'rb') as handle:\n\n        word2index = pickle.load(handle)\n        print(\"word2index dictionary loaded\", type(word2index))\n    # Load reverse dictionary  \n    model_path = Model.get_model_path(model_name = INDEX2WORD)\n    with open(model_path, 'rb') as handle:\n        index2word = pickle.load(handle)\n        print(\"index2word dictionary loaded\",type(index2word))\n    # Load stopword_set    \n    model_path = Model.get_model_path(model_name = STOPWORD_SET)\n    with open(model_path, 'rb') as handle:\n        stopword_set = pickle.load(handle)\n        print(\"stopword_set loaded\", type(stopword_set))\n    # Load index2tgt for reverse lookup of prediction\n    model_path = Model.get_model_path(model_name = INDEX2TGT)\n    with open(model_path, 'rb') as handle:\n        index2tgt = pickle.load(handle)\n        print(\"index2tgt loaded\", type(index2tgt))\n        \n\ndef run(raw_data):\n    import pandas as pd\n    try:\n        # Read json input\n        data = json.loads(raw_data)['data']\n        dataf = pd.read_json(data, orient='records')\n        holdout_df = dataf.copy()\n        \n        ## parse test data to list\n        interim_data_list = holdout_df['OPEN_FMR_TXT'].tolist()\n        \n        def replace_characters_and_stops(input_sentence, to_be_replaced, replacement_string, stops):\n            # input_sentence is a sentence from a list. to_be_replaced is a string, replacement_string is the unicode replacement. \n            # first we replace special characters with space, then trim long space to one space, then convert to lower case.\n            working_sentence = input_sentence.translate ({ord(c): replacement_string for c in to_be_replaced})\n            fixed_trimmed = re.sub('\\s+', ' ', working_sentence).strip().lower()\n            filtered_sentence = ' '.join([word for word in fixed_trimmed.split() if word not in stops])\n            \n            return filtered_sentence\n        \n        cleaned_data = [] \n        for record in interim_data_list:\n            fixed_record = replace_characters_and_stops(record,  '!@#$%^&*()[]{};:,./<>?\\|`~-=_+', ' ', stopword_set)\n            cleaned_data.append(fixed_record) \n        #print(cleaned_data)\n        # Need nltk.word_tokenize to tokenize each word, then map that word to my dictionary word2index[word] to find it's value.\n        num_recs = len(cleaned_data)\n        MAX_SENTENCE_LENGTH = 200 # The model is trained to accept this input length.\n        X = np.empty((num_recs, ), dtype=list)\n        \n        i = 0\n        nltk.download('punkt')\n        for idx, sentence in enumerate(cleaned_data):\n            \n            words = nltk.word_tokenize(''.join(sentence))\n            \n            seqs = []\n            \n            for word in words:\n                if word in word2index:\n                    seqs.append(word2index[word])\n                    \n                else:\n                    seqs.append(word2index['UNK'])\n                    \n            X[i] = seqs\n            i += 1\n        \n        #Zero pad each sequence at front to get sequence length MAX_SENTENCE_LENGTH\n        \n        X = tf.keras.preprocessing.sequence.pad_sequences(X[X !=np.array(None)].tolist(), maxlen=MAX_SENTENCE_LENGTH)\n        \n        # Make prediction here\n        predicted_val3 = model.predict(X)\n        \n        # Reverse lookup back to key.\n        value3_for_reverse_lookup = predicted_val3.argmax(axis=1)\n        predicted_ata_code3 = np.vectorize(index2tgt.get)(value3_for_reverse_lookup)   \n        zipped = list(zip(interim_data_list, predicted_ata_code3.tolist()))\n        df = pd.DataFrame(zipped, columns = ['OPEN_FMR_TXT', 'PREDICTED_OPEN_ORIGINAL_ATA']) \n        df2json = df.to_json(orient='records')       \n        return df2json\n\n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Writing /databricks/driver/tmp/score.py\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["%sh ls -lrt /databricks/driver/tmp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 405604\n-rw-r--r-- 1 root root 408866044 Oct 18 16:01 cnn_20191013-021616_ckpt.h5\n-rw-r--r-- 1 root root   3210328 Oct 18 16:01 word2index.pickle\n-rw-r--r-- 1 root root   3210328 Oct 18 16:01 index2word.pickle\n-rw-r--r-- 1 root root      8506 Oct 18 16:01 stopword_set.pickle\n-rw-r--r-- 1 root root     17492 Oct 18 16:01 index2tgt.pickle\n-rw-r--r-- 1 root root       611 Oct 18 16:08 env_tf20.yml\n-rw-r--r-- 1 root root      4582 Oct 18 16:08 score.py\n</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["####Define a Container Image\nWe're going to deploy the web service as a container, so we need to define a container image that includes our scoring file and denvironment dependencies."],"metadata":{}},{"cell_type":"code","source":["%sh ls -lrt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 28\ndrwxr-xr-x 2 root root 4096 Oct 18 15:50 conf\ndrwxr-xr-x 3 root root 4096 Oct 18 15:55 eventlogs\n-rw-r--r-- 1 root root  734 Oct 18 15:55 derby.log\ndrwxr-xr-x 2 root root 4096 Oct 18 16:00 ganglia\ndrwxr-xr-x 2 root root 4096 Oct 18 16:00 logs\n-rw-r--r-- 1 root root  611 Oct 18 16:08 env_tf20.yml\ndrwxr-xr-x 2 root root 4096 Oct 18 16:08 tmp\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["%sh cp /databricks/driver/tmp/score.py ./"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["%sh cp /databricks/driver/tmp/env_tf20.yml ./"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["Define a container image"],"metadata":{}},{"cell_type":"code","source":["from azureml.core.image import ContainerImage\n\nimage_config = ContainerImage.image_configuration(execution_script = \"score.py\",\n                                                  runtime = \"python\",\n                                                  conda_file = \"env_tf20.yml\",\n                                                  description = \"Container image for text classification\",\n                                                  tags = {\"data\": \"texts\", \"type\": \"classification\"}\n                                                 )\nprint(image_config.description, \"defined.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Container image for text classification defined.\n</div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["####Define the web service deployment configurationÂ¶\nWe're going to deploy the containerized web service in the Azure Container Instance (ACI) service, so we need to specify the deployment configuration."],"metadata":{}},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice\n\naci_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 4, \n                                               tags = {\"data\": \"texts\", \"type\": \"classification\"},\n                                               description = 'texts classification service')\nprint(aci_config.description, \"defined.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">texts classification service defined.\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["####Deploy the web service\nNow we're ready to deploy the model. The deployment process includes the following steps:\n\n<li>Register the model file in the Azure Machine Learning service (this also uploads the local model file to your Azure Machine Learning service so it can be deployed to a container)</li>\n<li>Create a container image for the web service, based on the configuration specified previously. This image will be used to instantiate the service.</li>\n<li>Create a service by deploying the container image (in this case to ACI - other hosts are available!)</li>\n<li>Verify the status of the deployed service.</li>\nThis will take some time. When deployment has completed successfully, you'll see a status of **Healthy**."],"metadata":{}},{"cell_type":"code","source":["%sh cat /databricks/driver/.azureml/config.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#34;Id&#34;: null, &#34;Scope&#34;: &#34;/subscriptions/b81e8c4c-2584-4aa7-8b10-bd1099cf015d/resourceGroups/tf20_deploy_rg4/providers/Microsoft.MachineLearningServices/workspaces/aa_runtest_aml_workspace_demo&#34;}</div>"]}}],"execution_count":44},{"cell_type":"code","source":["print(ws.name, ws.resource_group, ws.location, sep = '\\n')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">aa_runtest_aml_workspace_demo\ntf20_deploy_rg4\neastus2\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["#### Register Model\nWe have to register the model and its assets.The model registry is a way to store and organize your trained models in the Azure cloud. Models are registered in your Azure Machine Learning service workspace. The model can be trained using Azure Machine Learning, or another service. In our case, we have a LSTM model, and its accompanying assets such as dictionaries and a stopword list. To register a model from file, use the following code:"],"metadata":{}},{"cell_type":"code","source":["from azureml.core.model import Model\n\nmodel = Model.register(model_path = MODEL_PATH,\n                       model_name = MODEL,\n                       tags = {'type': \"cnn\", 'target': \"ATA_CODE\"},\n                       description = \"CNN model to predict ATA_CODE\",\n                       workspace = ws)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model cnn_20191013-021616_ckpt.h5\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["word2index_model = Model.register(model_path = WORD2INDEX_PATH,\n                       model_name = WORD2INDEX,\n                       tags = {'type': \"dict\", 'target': \"idx\"},\n                       description = \"word2index dictionary for tokenize input words as tokens and map to numeric value as index\",\n                       workspace = ws)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model word2index.pickle\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["index2word_model = Model.register(model_path =INDEX2WORD_PATH,\n                       model_name = INDEX2WORD,\n                       tags = {'type': \"dict\", 'target': \"idx\"},\n                       description = \"index2word dictionary for reverse lookup of input\",\n                       workspace = ws)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model index2word.pickle\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["stopword_set_model = Model.register(model_path =STOPWORD_SET_PATH,\n                       model_name = STOPWORD_SET,\n                       tags = {'type': \"set\", 'target': \"words\"},\n                       description = \"stopwords list including nltk and customized\",\n                       workspace = ws)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model stopword_set.pickle\n</div>"]}}],"execution_count":50},{"cell_type":"code","source":["index2tgt_model = Model.register(model_path =INDEX2TGT_PATH,\n                       model_name = INDEX2TGT,\n                       tags = {'type': \"dict\", 'target': \"words\"},\n                       description = \"reverse lookup dictionary for target ATA_CODE\",\n                       workspace = ws)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model index2tgt.pickle\n</div>"]}}],"execution_count":51},{"cell_type":"code","source":["for key,value in ws.models.items():\n    print(\"Name:\", key,\"\\tVersion:\", value.version)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Name: index2tgt.pickle \tVersion: 1\nName: stopword_set.pickle \tVersion: 1\nName: index2word.pickle \tVersion: 1\nName: word2index.pickle \tVersion: 1\nName: cnn_20191013-021616_ckpt.h5 \tVersion: 1\n</div>"]}}],"execution_count":52},{"cell_type":"markdown","source":["#### Key to Creating Docker Image - azureml.core.image Library\nMake sure `score.py` and `myenv.yml` are in same or current directory\n\nDeployed models are packaged as an image. The image contains the dependencies needed to run the model.\n\nFor Azure Container Instance, Azure Kubernetes Service, and Azure IoT Edge deployments, the `azureml.core.image.ContainerImage` class is used to create an image configuration. The image configuration is then used to create a new Docker image.\n\nThe following code demonstrates how to create a new image configuration:"],"metadata":{}},{"cell_type":"code","source":["%sh ls -lrt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 36\ndrwxr-xr-x 2 root root 4096 Oct 18 15:50 conf\ndrwxr-xr-x 3 root root 4096 Oct 18 15:55 eventlogs\n-rw-r--r-- 1 root root  734 Oct 18 15:55 derby.log\ndrwxr-xr-x 2 root root 4096 Oct 18 16:00 logs\ndrwxr-xr-x 2 root root 4096 Oct 18 16:08 tmp\n-rw-r--r-- 1 root root 4582 Oct 18 16:09 score.py\n-rw-r--r-- 1 root root  611 Oct 18 16:09 env_tf20.yml\ndrwxr-xr-x 2 root root 4096 Oct 18 16:15 ganglia\n</div>"]}}],"execution_count":54},{"cell_type":"code","source":["import time"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":55},{"cell_type":"code","source":["from azureml.core.image import ContainerImage\ntic = time.clock()\nimage_config = ContainerImage.image_configuration(execution_script = \"score.py\",\n                                                  runtime = \"python\",\n                                                  conda_file = \"env_tf20.yml\",\n                                                  description = \"CNN model to predict ATA_CODE\",\n                                                  tags = {'type': \"cnn\", 'target': \"ATA_CODE\"}\n                                                 )\nimage = ContainerImage.create(name = \"ataclassification\" + \".image\",\n                              # this is the model object\n                              models = [model, word2index_model, index2word_model, stopword_set_model, index2tgt_model],\n                              image_config = image_config,\n                              workspace = ws)\nimage.wait_for_creation(show_output = True)\ntoc = time.clock()\nprint(\"execution time in seconds: \", toc - tic )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating image\nRunning......................................................\nSucceeded\nImage creation operation finished for image ataclassification.image:1, operation &#34;Succeeded&#34;\nexecution time in seconds:  1.3451129999999978\n</div>"]}}],"execution_count":56},{"cell_type":"markdown","source":["#### Key to Image Deployment - azureml.core.webservice Library\nDeploy image as web service on Azure Container Instance (ACI) Note that the service creation can take few minutes.\n\nUse Azure Container Instances for deploying your models as a web service if one or more of the following conditions is true:\n\n1. You need to quickly deploy and validate your model.\n2. You are testing a model that is under development. \n\n| Method    | Note |\n| ----------- | ----------- |\n| `deploy_from_image`      | You must register the model and create an image before using this method.       |\n| `deploy`   | When using this method, you do not need to register the model or create the image. However you cannot control the name of the model or image, or associated tags and descriptions.        |\n| `deploy_from_model`   | When using this method, you do not need to create an image. But you do not have control over the name of the image that is created.        |\n\nThis example uses deploy_from_image."],"metadata":{}},{"cell_type":"code","source":["# Azure Container Service (ACI) Name\nACI_SERVICE_NAME = \"atacode\" + '-aciservice'\n\n# Azure Kubernetes Service (AKS) Name\nAKS_SERVICE_NAME = \"atacode\" + '-aksservice'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":58},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\n\naci_service_name = ACI_SERVICE_NAME.lower()\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 4, \n                                               tags = {'type': \"cnn\", 'target': \"ATA_CODE\"}, \n                                               description = \"CNN model to predict ATA_CODE\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"code","source":["#Let's see if we have an ACI web service already running in the workspace\naci_service = \"\"\nfor aci in AciWebservice.list(workspace=ws):\n    if (aci.compute_type == \"ACI\"):\n        if (aci.name == aci_service_name): \n            aci_service = aci\n            print(\"Existing ACI Service name:\", aci_service.name)\n        else:\n            print(\"No service by the name of **\"+aci_service_name+\"** exists!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["%%time\n# We update the image if service exists or create a new service if doesnt exist\nif (aci_service == \"\"):\n    aci_service = AciWebservice.deploy_from_image(deployment_config = aciconfig,\n                                           image = image,\n                                           name = aci_service_name,\n                                           workspace = ws)\n    aci_service.wait_for_deployment(True)\n    print(aci_service.state)\nelse:\n    aci_service.update(image=image)\n    aci_service.wait_for_deployment(True)\n    print(aci_service.state)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Running.............................\nSucceededACI service creation operation finished, operation &#34;Succeeded&#34;\nHealthy\nCPU times: user 477 ms, sys: 182 ms, total: 659 ms\nWall time: 2min 29s\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["print(aci_service.get_logs())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2019-10-18T16:28:23,431156716+00:00 - gunicorn/run \n2019-10-18T16:28:23,431156716+00:00 - rsyslog/run \n2019-10-18T16:28:23,431854716+00:00 - iot-server/run \n2019-10-18T16:28:23,695078917+00:00 - nginx/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2019-10-18T16:28:24,344527166+00:00 - iot-server/finish 1 0\n2019-10-18T16:28:24,354978170+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http://127.0.0.1:31311 (12)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 47\nInitializing logger\nStarting up app insights client\nStarting up request id generator\nStarting up app insight hooks\nInvoking user&#39;s init function\n2019-10-18 16:28:36,025 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}, switching offline: False\n2019-10-18 16:28:36,025 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-10-18 16:28:36,025 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n2019-10-18 16:28:36,025 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-10-18 16:28:36,026 | azureml.core.model | DEBUG | Found model path at azureml-models/cnn_20191013-021616_ckpt.h5/1/cnn_20191013-021616_ckpt.h5\n2019-10-18 16:28:36.053078: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-10-18 16:28:36.067177: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294685000 Hz\n2019-10-18 16:28:36.067652: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557bc1fc8cb0 executing computations on platform Host. Devices:\n2019-10-18 16:28:36.067758: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\nModel Loaded\n2019-10-18 16:28:38,325 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}, switching offline: False\n2019-10-18 16:28:38,325 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-10-18 16:28:38,325 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n2019-10-18 16:28:38,326 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-10-18 16:28:38,326 | azureml.core.model | DEBUG | Found model path at azureml-models/word2index.pickle/1/word2index.pickle\nword2index dictionary loaded\n&lt;class &#39;dict&#39;&gt;\n2019-10-18 16:28:38,373 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}, switching offline: False\n2019-10-18 16:28:38,373 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-10-18 16:28:38,374 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n2019-10-18 16:28:38,374 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-10-18 16:28:38,374 | azureml.core.model | DEBUG | Found model path at azureml-models/index2word.pickle/1/index2word.pickle\nindex2word dictionary loaded\n&lt;class &#39;dict&#39;&gt;\n2019-10-18 16:28:38,406 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}, switching offline: False\n2019-10-18 16:28:38,406 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-10-18 16:28:38,406 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n2019-10-18 16:28:38,406 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-10-18 16:28:38,407 | azureml.core.model | DEBUG | Found model path at azureml-models/stopword_set.pickle/1/stopword_set.pickle\nstopword_set loaded\n&lt;class &#39;set&#39;&gt;\n2019-10-18 16:28:38,407 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}, switching offline: False\n2019-10-18 16:28:38,407 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\n2019-10-18 16:28:38,408 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException RunEnvironmentException:\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\n\tInnerException None\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n\tErrorResponse \n{\n    &#34;error&#34;: {\n        &#34;message&#34;: &#34;Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.&#34;\n    }\n}\n2019-10-18 16:28:38,408 | azureml.core.model | DEBUG | version is None. Latest version is 1\n2019-10-18 16:28:38,409 | azureml.core.model | DEBUG | Found model path at azureml-models/index2tgt.pickle/1/index2tgt.pickle\nindex2tgt loaded\n&lt;class &#39;dict&#39;&gt;\nUsers&#39;s init has completed successfully\nScoring timeout setting is not found. Use default timeout: 3600000 ms\n\n</div>"]}}],"execution_count":62},{"cell_type":"code","source":["print(aci_service.name)\nprint(aci_service.state)\nprint(aci_service.location)\nprint(aci_service.image_id)\nprint(aci_service.scoring_uri)\nprint(aci_service.description)\nprint(aci_service.tags)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">atacode-aciservice\nHealthy\neastus2\nataclassification.image:1\nhttp://3b568015-5459-4ee8-a943-1a1a1b4e99b6.eastus2.azurecontainer.io/score\nCNN model to predict ATA_CODE\n{&#39;type&#39;: &#39;cnn&#39;, &#39;target&#39;: &#39;ATA_CODE&#39;}\n</div>"]}}],"execution_count":63},{"cell_type":"code","source":["print('web service hosted in ACI:', aci_service.scoring_uri)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">web service hosted in ACI: http://3b568015-5459-4ee8-a943-1a1a1b4e99b6.eastus2.azurecontainer.io/score\n</div>"]}}],"execution_count":64},{"cell_type":"code","source":["test_sample = json.dumps({\"data\": Xtest.to_json(orient='records')})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":65},{"cell_type":"code","source":["prediction = aci_service.run(input_data = test_sample)\nprint(prediction)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[{&#34;OPEN_FMR_TXT&#34;:&#34;5-983NEF-C                          REF.2530DN                  *NEF* GALLEY CART DOORS\\/HANDLES\\/DIVIDERS                         AFT GALLEY RAIL NEEDS TO BE REATTACHED.                         MEL AUTHORIZED BY 317887&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;2530&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;A\\/C REQUIRES ETOPS PRE-DEPARTURE CK&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ETOPS PDC REQUIRED&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ZIPPER ON CARGO LINER BROKEN ON P9 6R&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;5010&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;6-601NEF-C                          REF.4420AN                  *NEF* PSGR AUDIO\\/VIDEO..SYSTEM\\/ZONE.....                         IFE SYSTEM INOP SCREEN STUCK ON INITIALIZING                    MEL AUTHORIZED BY 190545&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;4420&#34;}]\n</div>"]}}],"execution_count":66},{"cell_type":"markdown","source":["#### Key to Deploying image as web service on Azure Kubernetes (AKS) - azureml.core.webservice and azureml.core.compute Libraries\n\nTo deploy your model as a high-scale production web service, use Azure Kubernetes Service (AKS). You can use an existing AKS cluster or create a new one using the Azure Machine Learning SDK, CLI, or the Azure portal.\n\nCreating an AKS cluster is a one time process for your workspace. You can reuse this cluster for multiple deployments. If you delete the cluster, then you must create a new cluster the next time you need to deploy.\n\nAzure Kubernetes Service provides the following capabilities:\n\n1. Autoscaling\n2. Logging\n3. Model data collection\n4. Fast response times for your web services\n5. TLS termination\n6. Authentication"],"metadata":{}},{"cell_type":"code","source":["from azureml.core.compute import AksCompute, ComputeTarget\nfrom azureml.core.webservice import AksWebservice"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":68},{"cell_type":"code","source":["#Let's see first if we have a compute cluster available in the workspace\naks_name = 'atacode-aks'\naks_target = \"\"\naks_service_name = AKS_SERVICE_NAME.lower() \naks_service = \"\"\n\nfor aks in AksCompute.list(ws):\n    if (aks.name == aks_name): \n        aks_target = aks\n        print(\"Existing Cluster name: \", aks_target.name)\n\nfor akss in AksWebservice.list(ws):\n      if (akss.name == aks_service_name): \n        aks_service = akss\n        print(\"Existing AKS Web Service name: \", aks_service.name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":69},{"cell_type":"code","source":["%%time\nif (aks_service == \"\"):\n    # Set the AKS Cluster configuration\n    prov_config = AksCompute.provisioning_configuration(agent_count=6, vm_size=\"Standard_DS2_v2\", ssl_cname=None, \n                                                        ssl_cert_pem_file=None, ssl_key_pem_file=None, \n                                                        location=\"EastUs2\")\n\n    # Create the cluster\n    aks_target = ComputeTarget.create(workspace = ws, \n                                     name = aks_name, \n                                     provisioning_configuration = prov_config)\n\n    aks_target.wait_for_completion(show_output = True)\n    print(aks_target.provisioning_errors)\n\n    #Set the web service configuration\n    aks_config = AksWebservice.deploy_configuration(autoscale_enabled=True, autoscale_min_replicas=3, \n                                                autoscale_max_replicas=10, autoscale_refresh_seconds=None, \n                                                autoscale_target_utilization=80, collect_model_data=None, \n                                                cpu_cores=None, memory_gb=None, enable_app_insights=True, \n                                                scoring_timeout_ms=None, replica_max_concurrent_requests=None, \n                                                num_replicas=None, primary_key=None, secondary_key=None, \n                                                tags = {'type': \"cnn\", 'target': \"ATA_CODE\"}, \n                                                description=\"AKS Service\")\n\n    # Create the Web Service\n    aks_service = AksWebservice.deploy_from_image(workspace = ws, \n                                           name = aks_service_name,\n                                           image = image,\n                                           deployment_config = aks_config,\n                                           deployment_target = aks_target)\n    \n    aks_service.wait_for_deployment(show_output = True)\n    print(aks_service.state)\n    \nelse:\n    aks_service.update(image=image)\n    aks_service.wait_for_deployment(show_output = True)\n    print(aks_service.state)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating.................................................................................................................................................................................................\nSucceededProvisioning operation finished, operation &#34;Succeeded&#34;\nNone\nRunning................\nSucceededAKS service creation operation finished, operation &#34;Succeeded&#34;\nHealthy\nCPU times: user 3.44 s, sys: 1.08 s, total: 4.52 s\nWall time: 18min 25s\n</div>"]}}],"execution_count":70},{"cell_type":"code","source":["print(\"Name:\", aks_target.name)\nprint(\"Agent Count:\", aks_target.agent_count)\nprint(\"VM Size:\", aks_target.agent_vm_size)\nprint(\"Location:\", aks_target.location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Name: atacode-aks\nAgent Count: 6\nVM Size: STANDARD_DS2_V2\nLocation: eastus2\n</div>"]}}],"execution_count":71},{"cell_type":"code","source":["print(aks_service.name)\nprint(aks_service.state)\nprint(aks_service.image_id)\nprint(aks_service.scoring_uri)\nprint(aks_service.get_keys()[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">atacode-aksservice\nHealthy\nataclassification.image:1\nhttp://137.116.85.79:80/api/v1/service/atacode-aksservice/score\n7AzZnS1imhCHjnsahsHO9woeFrYm1mlh\n</div>"]}}],"execution_count":72},{"cell_type":"code","source":["print(aks_service.get_keys())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&#39;7AzZnS1imhCHjnsahsHO9woeFrYm1mlh&#39;, &#39;oh1by1Is1h7L7hpD0WrIvMPklAUwu09c&#39;)\n</div>"]}}],"execution_count":73},{"cell_type":"code","source":["aks_service"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[63]: AksWebservice(workspace=Workspace.create(name=&#39;aa_runtest_aml_workspace_demo&#39;, subscription_id=&#39;b81e8c4c-2584-4aa7-8b10-bd1099cf015d&#39;, resource_group=&#39;tf20_deploy_rg4&#39;), name=atacode-aksservice, image_id=ataclassification.image:1, compute_type=AKS, state=Healthy, scoring_uri=http://137.116.85.79:80/api/v1/service/atacode-aksservice/score, tags={&#39;type&#39;: &#39;cnn&#39;, &#39;target&#39;: &#39;ATA_CODE&#39;}, properties={})</div>"]}}],"execution_count":74},{"cell_type":"code","source":["print('web service hosted in AKS:', aks_service.scoring_uri)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">web service hosted in AKS: http://137.116.85.79:80/api/v1/service/atacode-aksservice/score\n</div>"]}}],"execution_count":75},{"cell_type":"code","source":["test_sample"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[65]: &#39;{&#34;data&#34;: &#34;[{\\\\&#34;LOG_ID\\\\&#34;:\\\\&#34;878-34701643-20180521\\\\&#34;,\\\\&#34;OPEN_DATE\\\\&#34;:1526860800000,\\\\&#34;FLEET\\\\&#34;:\\\\&#34;A320\\\\&#34;,\\\\&#34;AIRLINE\\\\&#34;:\\\\&#34;AA\\\\&#34;,\\\\&#34;NOSE\\\\&#34;:\\\\&#34;878\\\\&#34;,\\\\&#34;OPEN_FMR_TXT\\\\&#34;:\\\\&#34;5-983NEF-C                          REF.2530DN                  *NEF* GALLEY CART DOORS\\\\\\\\/HANDLES\\\\\\\\/DIVIDERS                         AFT GALLEY RAIL NEEDS TO BE REATTACHED.                         MEL AUTHORIZED BY 317887\\\\&#34;,\\\\&#34;OPEN_ORIGINAL_ATA\\\\&#34;:\\\\&#34;2530\\\\&#34;,\\\\&#34;__index_level_0__\\\\&#34;:937801},{\\\\&#34;LOG_ID\\\\&#34;:\\\\&#34;861-34683012-20180424\\\\&#34;,\\\\&#34;OPEN_DATE\\\\&#34;:1524528000000,\\\\&#34;FLEET\\\\&#34;:\\\\&#34;A320\\\\&#34;,\\\\&#34;AIRLINE\\\\&#34;:\\\\&#34;AA\\\\&#34;,\\\\&#34;NOSE\\\\&#34;:\\\\&#34;861\\\\&#34;,\\\\&#34;OPEN_FMR_TXT\\\\&#34;:\\\\&#34;A\\\\\\\\/C REQUIRES ETOPS PRE-DEPARTURE CK\\\\&#34;,\\\\&#34;OPEN_ORIGINAL_ATA\\\\&#34;:\\\\&#34;1210\\\\&#34;,\\\\&#34;__index_level_0__\\\\&#34;:918120},{\\\\&#34;LOG_ID\\\\&#34;:\\\\&#34;7BS-35517002-20180509\\\\&#34;,\\\\&#34;OPEN_DATE\\\\&#34;:1525824000000,\\\\&#34;FLEET\\\\&#34;:\\\\&#34;B777\\\\&#34;,\\\\&#34;AIRLINE\\\\&#34;:\\\\&#34;AA\\\\&#34;,\\\\&#34;NOSE\\\\&#34;:\\\\&#34;7BS\\\\&#34;,\\\\&#34;OPEN_FMR_TXT\\\\&#34;:\\\\&#34;ETOPS PDC REQUIRED\\\\&#34;,\\\\&#34;OPEN_ORIGINAL_ATA\\\\&#34;:\\\\&#34;1210\\\\&#34;,\\\\&#34;__index_level_0__\\\\&#34;:788529},{\\\\&#34;LOG_ID\\\\&#34;:\\\\&#34;390-33945100-20180701\\\\&#34;,\\\\&#34;OPEN_DATE\\\\&#34;:1530403200000,\\\\&#34;FLEET\\\\&#34;:\\\\&#34;B767\\\\&#34;,\\\\&#34;AIRLINE\\\\&#34;:\\\\&#34;AA\\\\&#34;,\\\\&#34;NOSE\\\\&#34;:\\\\&#34;390\\\\&#34;,\\\\&#34;OPEN_FMR_TXT\\\\&#34;:\\\\&#34;ZIPPER ON CARGO LINER BROKEN ON P9 6R\\\\&#34;,\\\\&#34;OPEN_ORIGINAL_ATA\\\\&#34;:\\\\&#34;5010\\\\&#34;,\\\\&#34;__index_level_0__\\\\&#34;:203513},{\\\\&#34;LOG_ID\\\\&#34;:\\\\&#34;3LL-34713941-20180606\\\\&#34;,\\\\&#34;OPEN_DATE\\\\&#34;:1528243200000,\\\\&#34;FLEET\\\\&#34;:\\\\&#34;B737\\\\&#34;,\\\\&#34;AIRLINE\\\\&#34;:\\\\&#34;AA\\\\&#34;,\\\\&#34;NOSE\\\\&#34;:\\\\&#34;3LL\\\\&#34;,\\\\&#34;OPEN_FMR_TXT\\\\&#34;:\\\\&#34;6-601NEF-C                          REF.4420AN                  *NEF* PSGR AUDIO\\\\\\\\/VIDEO..SYSTEM\\\\\\\\/ZONE.....                         IFE SYSTEM INOP SCREEN STUCK ON INITIALIZING                    MEL AUTHORIZED BY 190545\\\\&#34;,\\\\&#34;OPEN_ORIGINAL_ATA\\\\&#34;:\\\\&#34;4420\\\\&#34;,\\\\&#34;__index_level_0__\\\\&#34;:399489}]&#34;}&#39;</div>"]}}],"execution_count":76},{"cell_type":"code","source":["Xtest.to_json(orient='records')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[66]: &#39;[{&#34;LOG_ID&#34;:&#34;878-34701643-20180521&#34;,&#34;OPEN_DATE&#34;:1526860800000,&#34;FLEET&#34;:&#34;A320&#34;,&#34;AIRLINE&#34;:&#34;AA&#34;,&#34;NOSE&#34;:&#34;878&#34;,&#34;OPEN_FMR_TXT&#34;:&#34;5-983NEF-C                          REF.2530DN                  *NEF* GALLEY CART DOORS\\\\/HANDLES\\\\/DIVIDERS                         AFT GALLEY RAIL NEEDS TO BE REATTACHED.                         MEL AUTHORIZED BY 317887&#34;,&#34;OPEN_ORIGINAL_ATA&#34;:&#34;2530&#34;,&#34;__index_level_0__&#34;:937801},{&#34;LOG_ID&#34;:&#34;861-34683012-20180424&#34;,&#34;OPEN_DATE&#34;:1524528000000,&#34;FLEET&#34;:&#34;A320&#34;,&#34;AIRLINE&#34;:&#34;AA&#34;,&#34;NOSE&#34;:&#34;861&#34;,&#34;OPEN_FMR_TXT&#34;:&#34;A\\\\/C REQUIRES ETOPS PRE-DEPARTURE CK&#34;,&#34;OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;,&#34;__index_level_0__&#34;:918120},{&#34;LOG_ID&#34;:&#34;7BS-35517002-20180509&#34;,&#34;OPEN_DATE&#34;:1525824000000,&#34;FLEET&#34;:&#34;B777&#34;,&#34;AIRLINE&#34;:&#34;AA&#34;,&#34;NOSE&#34;:&#34;7BS&#34;,&#34;OPEN_FMR_TXT&#34;:&#34;ETOPS PDC REQUIRED&#34;,&#34;OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;,&#34;__index_level_0__&#34;:788529},{&#34;LOG_ID&#34;:&#34;390-33945100-20180701&#34;,&#34;OPEN_DATE&#34;:1530403200000,&#34;FLEET&#34;:&#34;B767&#34;,&#34;AIRLINE&#34;:&#34;AA&#34;,&#34;NOSE&#34;:&#34;390&#34;,&#34;OPEN_FMR_TXT&#34;:&#34;ZIPPER ON CARGO LINER BROKEN ON P9 6R&#34;,&#34;OPEN_ORIGINAL_ATA&#34;:&#34;5010&#34;,&#34;__index_level_0__&#34;:203513},{&#34;LOG_ID&#34;:&#34;3LL-34713941-20180606&#34;,&#34;OPEN_DATE&#34;:1528243200000,&#34;FLEET&#34;:&#34;B737&#34;,&#34;AIRLINE&#34;:&#34;AA&#34;,&#34;NOSE&#34;:&#34;3LL&#34;,&#34;OPEN_FMR_TXT&#34;:&#34;6-601NEF-C                          REF.4420AN                  *NEF* PSGR AUDIO\\\\/VIDEO..SYSTEM\\\\/ZONE.....                         IFE SYSTEM INOP SCREEN STUCK ON INITIALIZING                    MEL AUTHORIZED BY 190545&#34;,&#34;OPEN_ORIGINAL_ATA&#34;:&#34;4420&#34;,&#34;__index_level_0__&#34;:399489}]&#39;</div>"]}}],"execution_count":77},{"cell_type":"code","source":["test_sample = json.dumps({\"data\": Xtest.to_json(orient='records')})\n\nprediction = aks_service.run(input_data = test_sample)\nprint(prediction)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[{&#34;OPEN_FMR_TXT&#34;:&#34;5-983NEF-C                          REF.2530DN                  *NEF* GALLEY CART DOORS\\/HANDLES\\/DIVIDERS                         AFT GALLEY RAIL NEEDS TO BE REATTACHED.                         MEL AUTHORIZED BY 317887&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;2530&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;A\\/C REQUIRES ETOPS PRE-DEPARTURE CK&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ETOPS PDC REQUIRED&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;1210&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;ZIPPER ON CARGO LINER BROKEN ON P9 6R&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;5010&#34;},{&#34;OPEN_FMR_TXT&#34;:&#34;6-601NEF-C                          REF.4420AN                  *NEF* PSGR AUDIO\\/VIDEO..SYSTEM\\/ZONE.....                         IFE SYSTEM INOP SCREEN STUCK ON INITIALIZING                    MEL AUTHORIZED BY 190545&#34;,&#34;PREDICTED_OPEN_ORIGINAL_ATA&#34;:&#34;4420&#34;}]\n</div>"]}}],"execution_count":78},{"cell_type":"markdown","source":["The model image successfully ran on Kubernetes cluster. \n\n### What we have accomplished:\n1. Register a custom-built machine learning model and its assets.\n2. Create an image of the model and its assets.\n3. Test the image deployment in ACI and Kubernetes using JSON input."],"metadata":{}},{"cell_type":"markdown","source":["#### Clean up\n\nTo delete a deployed web service, use `service.delete()`.\n\nTo delete an image, use `image.delete()`.\n\nTo delete a registered model, use `model.delete()`."],"metadata":{}},{"cell_type":"code","source":["%%time\naci_service.delete()\naks_service.delete()\naks_target.delete()\nimage.delete()\nmodel.delete()\nword2index_model.delete()\nindex2word_model.delete()\nstopword_set_model.delete()\nindex2tgt_model.delete()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CPU times: user 155 ms, sys: 21 ms, total: 176 ms\nWall time: 11.8 s\n</div>"]}}],"execution_count":81},{"cell_type":"code","source":["print(aks_service.state)\nprint(aci_service.state)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Deleting\nDeleting\n</div>"]}}],"execution_count":82},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":83}],"metadata":{"name":"3 TF20 AKS","notebookId":3011530239776096},"nbformat":4,"nbformat_minor":0}
